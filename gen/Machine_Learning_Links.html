<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">
<head>
   <meta http-equiv="content-type" content="text/html; charset=utf-8" />
   <meta name="author" content="Mars Gu" />
   <!--link href="http://kevinburke.bitbucket.org/markdowncss/markdown.css" rel="stylesheet"></link-->
   <link href="http://marsishandsome.github.io/template/github.css" rel="stylesheet"></link>
</head>
<body>
<style>#forkongithub a{background:#81BEF7;color:#fff;text-decoration:none;font-family:arial,sans-serif;text-align:center;font-weight:bold;padding:5px 40px;font-size:1rem;line-height:2rem;position:relative;transition:0.5s;}#forkongithub a:hover{background:#EB6F6F;color:#fff;}#forkongithub a::before,#forkongithub a::after{content:"";width:100%;display:block;position:absolute;top:1px;left:0;height:1px;background:#fff;}#forkongithub a::after{bottom:1px;top:auto;}@media screen and (min-width:100px){#forkongithub{position:fixed;display:block;top:0;left:0;width:200px;overflow:hidden;height:200px;z-index:9999;}#forkongithub a{width:200px;position:absolute;top:60px;left:-60px;transform:rotate(-45deg);-webkit-transform:rotate(-45deg);-ms-transform:rotate(-45deg);-moz-transform:rotate(-45deg);-o-transform:rotate(-45deg);box-shadow:4px 4px 4px rgba(0,0,0,0.8);}}</style><span id="forkongithub"><a href="https://github.com/marsishandsome/marsishandsome.github.com">Fork me on GitHub</a></span>
<h1>
  <a href="/">
    Mars的笔记
  </a>
  <a href="http://feeds.feedburner.com/marsishandsome">
    <img src="http://marsishandsome.github.io/template/rss.png" style="width:30px;height:30px;"/>
  </a>
</h1>
<h1 id="open-source">Open Source</h1>
<ul>
<li><a href="https://github.com/datumbox/datumbox-framework">datumbox-framework</a></li>
</ul>
<h1 id="course">Course</h1>
<ul>
<li><a href="http://www.st.ewi.tudelft.nl/~hauff/TI2736-B.html">TI2736-B: Big Data Processing</a><br /></li>
<li><a href="http://www.slideshare.net/hustwj/scaling-up-machine-learning-the-tutorial-kdd-2011-part-i-introduction?related=1">scaling-up-machine-learning</a></li>
</ul>
<h1 id="feature-learning">Feature Learning</h1>
<ul>
<li><a href="http://www.cs.stanford.edu/~acoates/papers/coatesng_nntot2012.pdf">Learning Feature Representations with K-means</a> by Adam Coates and Andrew Y. Ng<br /></li>
<li><a href="http://www.robots.ox.ac.uk/~vgg/publications/2011/Chatfield11/chatfield11.pdf">The devil is in the details: an evaluation of recent feature encoding methods</a> by Chatfield et. al.<br /></li>
<li><a href="http://web.stanford.edu/~acoates/papers/coateskarpathyng_nips2012.pdf">Emergence of Object-Selective Features in Unsupervised Feature Learning</a> by Coates, Ng<br /></li>
<li><a href="http://yann.lecun.com/exdb/publis/pdf/bengio-lecun-07.pdf">Scaling Learning Algorithms towards AI</a> Benjio &amp; LeCun</li>
</ul>
<h1 id="deep-learning">Deep Learning</h1>
<ul>
<li><a href="http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</a> by Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever and Ruslan Salakhutdinov<br /></li>
<li><a href="http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf">Understanding the difficulty of training deep feedforward neural networks</a> by Xavier Glorot and Yoshua Bengio<br /></li>
<li><a href="http://arxiv.org/pdf/1211.5063v2.pdf">On the difficulty of training Recurrent Neural Networks</a> by Razvan Pascanu, Tomas Mikolov and Yoshua Bengio<br /></li>
<li><a href="http://arxiv.org/abs/1502.03167">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a> by Sergey Ioffe and Christian Szegedy<br /></li>
<li><a href="http://arxiv.org/pdf/1404.7828v4.pdf">Deep Learning in Neural Networks: An Overview</a> by Jurgen Schmidhuber<br /></li>
<li><a href="http://arxiv.org/abs/1412.6544">Qualitatively characterizing neural network optimization problems</a> by Ian J. Goodfellow, Oriol Vinyals<br /></li>
<li><a href="http://www-etud.iro.umontreal.ca/~pascanur/papers/thesis.pdf">On Recurrent and Deep Neural Networks</a> Phd thesis of Razvan Pascanu<br /></li>
<li><a href="http://yann.lecun.com/exdb/publis/pdf/bengio-lecun-07.pdf">Scaling Learning Algorithms towards AI</a> by Yann LeCun and Yoshua Benjio<br /></li>
<li><a href="http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf">Efficient Backprop</a> by LeCun, Bottou et al<br /></li>
<li><a href="http://arxiv.org/abs/1502.04156">Towards Biologically Plausible Deep Learning</a> by Yoshua Bengio, Dong-Hyun Lee, Jorg Bornschein, Zhouhan Lin<br /></li>
<li><a href="http://www.cs.utoronto.ca/~ilya/pubs/ilya_sutskever_phd_thesis.pdf">Training Recurrent Neural Networks</a> Phd thesis of Ilya Sutskever<br /></li>
<li><a href="http://arxiv.org/pdf/1504.00641v1.pdf">A Probabilistic Theory of Deep Learning</a> by Ankit B. Patel, Tan Nguyen, Richard G. Baraniuk</li>
</ul>
<h1 id="scalable-machine-learning">Scalable Machine Learning</h1>
<ul>
<li><a href="http://online.liebertpub.com/doi/pdf/10.1089/big.2013.0010">Bring the Noise: Embracing Randomness is the Key to Scaling Up Machine Learning Algorithms</a> by Brian Delssandro<br /></li>
<li><a href="http://leon.bottou.org/publications/pdf/compstat-2010.pdf">Large Scale Machine Learning with Stochastic Gradient Descent</a> by Leon Bottou<br /></li>
<li><a href="http://papers.nips.cc/paper/3323-the-tradeoffs-of-large-scale-learning.pdf">The TradeOffs of Large Scale Learning</a> by Leon Bottou &amp; Olivier Bousquet<br /></li>
<li><a href="http://www.jmlr.org/papers/volume10/shi09a/shi09a.pdf">Hash Kernels for Structured Data</a> by Qinfeng Shi et. al.<br /></li>
<li><a href="http://arxiv.org/pdf/0902.2206.pdf">Feature Hashing for Large Scale Multitask Learning</a> by Weinberger et. al.<br /></li>
<li><a href="http://www.eecs.tufts.edu/~dsculley/papers/round-model-icml.pdf">Large-Scale Learning with Less RAM via Randomization</a> by a group of authors from Google</li>
</ul>
<h1 id="gradient-based-training">Gradient based Training</h1>
<ul>
<li><a href="http://arxiv.org/pdf/1206.5533v2.pdf">Practical Recommendations for Gradient-Based Training of Deep Architectures</a> by Yoshua Bengio<br /></li>
<li><a href="http://research.microsoft.com/pubs/192769/tricks-2012.pdf">Stochastic Gradient Descent Tricks</a> by L´eon Bottou</li>
</ul>
<h1 id="non-linear-units">Non Linear Units</h1>
<ul>
<li><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.165.6419&amp;rep=rep1&amp;type=pdf">Rectified Linear Units Improve Restricted Boltzmann Machines</a> by Nair &amp; Hinton<br /></li>
<li><a href="https://www.academia.edu/7826776/Mathematical_Intuition_for_Performance_of_Rectified_Linear_Unit_in_Deep_Neural_Networks">Mathematical Intuition for Performance of Rectified Linear Unit in Deep Neural Networks</a> by Alexandre Dalyec</li>
</ul>
<h1 id="interesting-blog-posts">Interesting blog posts</h1>
<ul>
<li><a href="https://karpathy.github.io/neuralnets/">Hacker's Guide to Neural Networks</a> by Andrej Karpathy<br /></li>
<li><a href="http://karpathy.github.io/2015/03/30/breaking-convnets/">Breaking Linear Classifiers on ImageNet</a> by Andrej Karpathy<br /></li>
<li><a href="http://benanne.github.io/2015/03/17/plankton.html">Classifying plankton with Deep Neural Networks</a><br /></li>
<li><a href="https://blogs.princeton.edu/imabandit/2015/03/20/deep-stuff-about-deep-learning/">Deep stuff about deep learning?</a><br /></li>
<li><a href="https://timdettmers.wordpress.com/2015/03/26/convolution-deep-learning/">Understanding Convolution in Deep Learning</a><br /></li>
<li><a href="http://yyue.blogspot.in/2015/01/a-brief-overview-of-deep-learning.html">A Brief Overview of Deep Learning</a> by Ilya Sutskever<br /></li>
<li><a href="http://erikbern.com/?p=589">Recurrent Neural Networks for Collaborative Filtering</a></li>
</ul>
<h1 id="interesting-courses">Interesting courses</h1>
<ul>
<li><a href="http://cs231n.stanford.edu/">CS231n: Convolutional Neural Networks for Visual Recognition</a> at Stanford by Andrej Karpathy<br /></li>
<li><a href="http://cs224d.stanford.edu/">CS224d: Deep Learning for Natural Language Processing</a> at Stanford by Richard Socher<br /></li>
<li><a href="http://www.cs.toronto.edu/~rsalakhu/STA4273_2015/">STA 4273H (Winter 2015): Large Scale Machine Learning</a> at Toronto by Russ Salakhutdinov<br /></li>
<li><a href="http://am207.org/">AM 207 Monte Carlo Methods, Stochastic Optimization</a> at Harvard by Verena Kaynig-Fittkau and Pavlos Protopapas<br /></li>
<li><a href="http://www.socher.org/index.php/DeepLearningTutorial/DeepLearningTutorial">ACL 2012 + NAACL 2013 Tutorial: Deep Learning for NLP (without Magic)</a> at NAACL 2013 by Richard Socher, Chris Manning and Yoshua Bengio<br /></li>
<li><a href="https://www.youtube.com/playlist?list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH">Video course on Deep Learning</a> by Hugo Larochelle</li>
</ul>
<p><a href="https://gist.github.com/debasishg/b4df1648d3f1776abdff">source</a></p>

<div id="disqus_thread"></div>
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES * * */
    var disqus_shortname = 'mars-notes';

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

</body>
</html>
