<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">
<head>
   <meta http-equiv="content-type" content="text/html; charset=utf-8" />
   <meta name="author" content="Mars Gu" />
   <!--link href="http://kevinburke.bitbucket.org/markdowncss/markdown.css" rel="stylesheet"></link-->
   <link href="https://gist.githubusercontent.com/andyferra/2554919/raw/2e66cabdafe1c9a7f354aa2ebf5bc38265e638e5/github.css" rel="stylesheet"></link>
</head>
<body>
<h1>
  <a href="/index.html">
    Mars的笔记
  </a>
  <a href="http://feeds.feedburner.com/marsishandsome">
    <img src="http://marsishandsome.github.io/template/rss.png" style="width:30px;height:30px;"/>
  </a>
</h1>
<h1 id="spark-r-">Spark R 架构分析</h1>
<p>SparkR项目最近已经merge到spark的master分支，将会在spark-1.4.0中正式发布。</p>
<p>我认为SparkR项目的意义在于：</p>
<ol>
<li>使用R语言的专业人士可以无缝地通过R语言来使用Spark<br /></li>
<li>在Spark上可以使用R的上千个package，避免重复发明轮子<br /></li>
<li>证明了Spark框架的可扩展性</li>
</ol>
<p>本篇主要来分析一下SparkR的架构。</p>
<h3 id="arcitecutre">Arcitecutre</h3>
<p>SparkR的架构类似于PySpark，Driver端除了一个JVM进程（包含一个SparkContext）外，还有起一个R的进程，这两个进程通过Socket进行通信，用户可以提交R语言代码，R的进程会执行这些R代码，当R代码调用Spark相关函数时，R进程会通过Socket触发JVM中的对应任务。</p>
<p>当R进程向JVM进程提交任务的时候，R会把子任务需要的环境(enclosing environment)进行打包，并发送到JVM的driver端。</p>
<p>通过R生成的RDD都会是RRDD类型，当触发RRDD的action时，Spark的执行器会开启一个R进程(worker.R)，执行器和R进程通过Socket进行通信。执行器会把任务和所需的环境发送给R进程，R进程会加载对应的package，执行任务，并返回结果。</p>
<p><img src="/images/spark_r_dataflow.png" width="1000px"></p>
<p>下面是介绍一些执行SparkR的详细流程：</p>
<ol>
<li>SparkSubmit判断是SparkR类型的任务，启动RRunner<br /></li>
<li>RRunner首先启动RBackend，然后再启动R进程执行用户的R脚本<br /></li>
<li>RBackend开启Socket Server，等待R进程链接<br /></li>
<li>R进程通过Socket连接到RBackend<br /></li>
<li>R脚本生成SparkContext，通过Socket通信，会在JVM中生成JavaSparkContext<br /></li>
<li>R脚本触发RRDD的action，通过Socket通信，Driver启动executor，并执行对于的任务<br /></li>
<li>在每个执行器中都会调用RRDD的compute函数，来计算RRDD中的数据<br /></li>
<li>compute函数首先开启Socket Server，等待R进程链接，然后启动一个R进程(worker.R)<br /></li>
<li>R进程通过Socket链接到执行器，读取执行器发送过来的任务，执行任务，并返回结果<br /></li>
<li>compute函数读取R进程发来的结果数据，并返回iterator</li>
</ol>
<p>1-6属于driver端，7-10属于executor端。</p>
<p><img src="/images/spark_r_workflow.png" width="1000px"></p>
<h3>可能的问题及改进</h3>
<ol>
<li>Standalone模式如何控制R进程的资源？<br /></li>
<li>JVM进程和R进程通过Socket进行通信，代价多高？可否用共享内存？</li>
</ol>
<h3 id="links">Links</h3>
<ul>
<li><a href="http://amplab-extras.github.io/SparkR-pkg/">SparkR 官网</a><br /></li>
<li><a href="https://github.com/amplab-extras/SparkR-pkg">SparkR Github</a><br /></li>
<li><a href="https://issues.apache.org/jira/browse/SPARK-5654">SparkR Apache JIRA</a><br /></li>
<li><a href="https://github.com/apache/spark/pull/5096">SparkR Github Pull Request</a></li>
</ul>
<h3 id="documents">Documents</h3>
<ul>
<li><a href="http://spark-summit.org/2014/talk/sparkr-interactive-r-programs-at-scale-2">SparkR: Interactive R programs at Scale</a> - Spark Summit 2014<br /></li>
<li><a href="https://www.youtube.com/watch?v=Y21t3Taw7i8">SparkR Enabling Interactive Data Science at Scale on Hadoop</a> - Hadoop Summit<br /></li>
<li><a href="https://www.youtube.com/watch?v=OxVIns6zvlk">AMP Camp 5: SparkR</a></li>
</ul>
<h3 id="details">Details</h3>
<ul>
<li><a href="http://blog.obeautifulcode.com/R/How-R-Searches-And-Finds-Stuff/">How R Searches and Finds Stuff</a></li>
</ul>

</body>
</html>
