<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">
<head>
   <meta http-equiv="content-type" content="text/html; charset=utf-8" />
   <meta name="author" content="Mars Gu" />

   <title>Mars的笔记</title>

   <!--jquery-->
   <script src="http://marsishandsome.github.io/libs/jquery/jquery-1.11.3.min.js"></script>

   <!--highlight-->
   <link rel="stylesheet" href="http://marsishandsome.github.io/libs/highlight/styles/androidstudio.css">
   <script src="http://marsishandsome.github.io/libs/highlight/highlight.pack.js"></script>

   <!--boostrap-->
   <link href="http://marsishandsome.github.io/libs/bootstrap/css/bootstrap.min.css" rel="stylesheet"></link>
   <link href="http://marsishandsome.github.io/libs/bootstrap/css/bootstrap-responsive.min.css" rel="stylesheet"></link>
   <script src="http://marsishandsome.github.io/libs/bootstrap/js/bootstrap.min.js"></script>

   <!--markdown-->
   <link href="http://marsishandsome.github.io/template/github.css" rel="stylesheet"></link>

</head>
<body>
<style>#forkongithub a{background:#81BEF7;color:#fff;text-decoration:none;font-family:arial,sans-serif;text-align:center;font-weight:bold;padding:5px 40px;font-size:1rem;line-height:2rem;position:relative;transition:0.5s;}#forkongithub a:hover{background:#EB6F6F;color:#fff;}#forkongithub a::before,#forkongithub a::after{content:"";width:100%;display:block;position:absolute;top:1px;left:0;height:1px;background:#fff;}#forkongithub a::after{bottom:1px;top:auto;}@media screen and (min-width:100px){#forkongithub{position:fixed;display:block;top:0;left:0;width:200px;overflow:hidden;height:200px;z-index:9999;}#forkongithub a{width:200px;position:absolute;top:60px;left:-60px;transform:rotate(-45deg);-webkit-transform:rotate(-45deg);-ms-transform:rotate(-45deg);-moz-transform:rotate(-45deg);-o-transform:rotate(-45deg);box-shadow:4px 4px 4px rgba(0,0,0,0.8);}}</style><span id="forkongithub"><a href="https://github.com/marsishandsome/marsishandsome.github.com">Fork me on GitHub</a></span>
<div class="row">
  <div class="span10 offset2">
    <h1>
      <a href="/">
        Mars的笔记
      </a>
      <a href="http://feeds.feedburner.com/marsishandsome">
        <img src="http://marsishandsome.github.io/template/rss.png" style="width:30px;height:30px;"/>
      </a>
    </h1>
<h1 id="spark-on-yarn">Spark on Yarn源码分析</h1>
<p>阅读本篇需要Yarn的基础，强烈推荐Yarn入门文章: <a href="http://my.oschina.net/u/1434348/blog/193374">YARN应用开发流程</a>。</p>
<p>本篇代码基于Spark-1.3.0。</p>
<h3 id="spark-on-yarn">Spark on Yarn调用流图</h3>
<p>Spark on Yarn支持两种部署模式：Cluster VS Client。<br />两种模式最大的区别在于Spark Driver的运行位置，Cluster模式下Driver运行在Application Master中，而Client模式下Driver运行在本地。</p>
<p>Yarn Cluster模式<br /><img src="http://blog.cloudera.com/wp-content/uploads/2014/05/spark-yarn-f31.png" /></p>
<p>Yarn Client模式<br /><img src="http://blog.cloudera.com/wp-content/uploads/2014/05/spark-yarn-f22.png" /></p>
<p>Spark利用AKKA位置透明的特性，使得这两种模式可以共用同一套代码。Spark on Yarn调用流图如下：<br /><img src="../../../images/spark_on_yarn_arch.png" /></p>
<h3 id="yarn-cluster">Yarn-Cluster模式代码分析</h3>
<p>1: Client<br />SparkSubmit是Spark程序的入口</p>
<pre class="sourceCode scala"><code class="sourceCode scala"><span class="kw">if</span> (isYarnCluster) {
    <span class="co">//启动Client类</span>
    childMainClass = <span class="st">&quot;org.apache.spark.deploy.yarn.Client&quot;</span>
    <span class="co">//...</span>
}</code></pre>
<p>Client.main</p>
<pre class="sourceCode scala"><code class="sourceCode scala"><span class="co">//读取参数</span>
<span class="kw">val</span> args = <span class="kw">new</span> <span class="fu">ClientArguments</span>(argStrings, sparkConf)
<span class="kw">new</span> <span class="fu">Client</span>(args, sparkConf).<span class="fu">run</span>()</code></pre>
<p>Client.run</p>
<pre class="sourceCode scala"><code class="sourceCode scala"><span class="co">//submit &amp; monitor application</span>
<span class="kw">val</span> (yarnApplicationState, finalApplicationStatus) = <span class="fu">monitorApplication</span>(<span class="fu">submitApplication</span>())</code></pre>
<p>1.1: SubmitApplication<br />Client.submitApplication</p>
<pre class="sourceCode scala"><code class="sourceCode scala"><span class="co">//新建一个Application</span>
<span class="kw">val</span> newApp = yarnClient.<span class="fu">createApplication</span>()
<span class="kw">val</span> newAppResponse = newApp.<span class="fu">getNewApplicationResponse</span>()
<span class="kw">val</span> appId = newAppResponse.<span class="fu">getApplicationId</span>()

<span class="co">//创建environment, java options以及启动AM的命令</span>
<span class="kw">val</span> containerContext = <span class="fu">createContainerLaunchContext</span>(newAppResponse)

<span class="co">//创建提交AM的Context，包括名字、队列、类型、内存、CPU及参数</span>
<span class="kw">val</span> appContext = <span class="fu">createApplicationSubmissionContext</span>(newApp, containerContext)

<span class="co">//向Yarn提交Application</span>
yarnClient.<span class="fu">submitApplication</span>(appContext)</code></pre>
<p>Client.createContainerLaunchContext</p>
<pre class="sourceCode scala"><code class="sourceCode scala"><span class="co">//创建environment, java options以及启动AM的命令</span>
<span class="kw">val</span> launchEnv = <span class="fu">setupLaunchEnv</span>(appStagingDir)
<span class="kw">val</span> amContainer = Records.<span class="fu">newRecord</span>(classOf[ContainerLaunchContext])
amContainer.<span class="fu">setLocalResources</span>(localResources)
amContainer.<span class="fu">setEnvironment</span>(launchEnv)

<span class="kw">val</span> amClass =
  <span class="kw">if</span> (isClusterMode) {
    Class.<span class="fu">forName</span>(<span class="st">&quot;org.apache.spark.deploy.yarn.ApplicationMaster&quot;</span>).<span class="fu">getName</span>
  } <span class="kw">else</span> {
    Class.<span class="fu">forName</span>(<span class="st">&quot;org.apache.spark.deploy.yarn.ExecutorLauncher&quot;</span>).<span class="fu">getName</span>
  }

<span class="co">// Command for the ApplicationMaster</span>
<span class="kw">val</span> commands = prefixEnv ++ Seq(
    YarnSparkHadoopUtil.<span class="fu">expandEnvironment</span>(Environment.<span class="fu">JAVA_HOME</span>) + <span class="st">&quot;/bin/java&quot;</span>, <span class="st">&quot;-server&quot;</span>
  ) ++
  javaOpts ++ amArgs ++
  Seq(
    <span class="st">&quot;1&gt;&quot;</span>, ApplicationConstants.<span class="fu">LOG_DIR_EXPANSION_VAR</span> + <span class="st">&quot;/stdout&quot;</span>,
    <span class="st">&quot;2&gt;&quot;</span>, ApplicationConstants.<span class="fu">LOG_DIR_EXPANSION_VAR</span> + <span class="st">&quot;/stderr&quot;</span>)
<span class="kw">val</span> printableCommands = commands.<span class="fu">map</span>(s =&gt; <span class="kw">if</span> (s == <span class="kw">null</span>) <span class="st">&quot;null&quot;</span> <span class="kw">else</span> s).<span class="fu">toList</span>
amContainer.<span class="fu">setCommands</span>(printableCommands)

<span class="co">// send the acl settings into YARN to control who has access via YARN interfaces</span>
<span class="kw">val</span> securityManager = <span class="kw">new</span> SecurityManager(sparkConf)
amContainer.<span class="fu">setApplicationACLs</span>(YarnSparkHadoopUtil.<span class="fu">getApplicationAclsForYarn</span>(securityManager))
<span class="fu">setupSecurityToken</span>(amContainer)
UserGroupInformation.<span class="fu">getCurrentUser</span>().<span class="fu">addCredentials</span>(credentials)</code></pre>
<p>Client.createApplicationSubmissionContext</p>
<pre class="sourceCode scala"><code class="sourceCode scala"><span class="co">//创建提交AM的Context，包括名字、队列、类型、内存、CPU及参数</span>
<span class="kw">val</span> appContext = newApp.<span class="fu">getApplicationSubmissionContext</span>
appContext.<span class="fu">setApplicationName</span>(args.<span class="fu">appName</span>)
appContext.<span class="fu">setQueue</span>(args.<span class="fu">amQueue</span>)
appContext.<span class="fu">setAMContainerSpec</span>(containerContext)
appContext.<span class="fu">setApplicationType</span>(<span class="st">&quot;SPARK&quot;</span>)
sparkConf.<span class="fu">getOption</span>(<span class="st">&quot;spark.yarn.maxAppAttempts&quot;</span>).<span class="fu">map</span>(_.<span class="fu">toInt</span>) <span class="kw">match</span> {
  <span class="kw">case</span> Some(v) =&gt; appContext.<span class="fu">setMaxAppAttempts</span>(v)
  <span class="kw">case</span> None =&gt; <span class="fu">logDebug</span>(<span class="st">&quot;spark.yarn.maxAppAttempts is not set. &quot;</span> +
      <span class="st">&quot;Cluster&#39;s default value will be used.&quot;</span>)
}
<span class="kw">val</span> capability = Records.<span class="fu">newRecord</span>(classOf[Resource])
capability.<span class="fu">setMemory</span>(args.<span class="fu">amMemory</span> + amMemoryOverhead)
capability.<span class="fu">setVirtualCores</span>(args.<span class="fu">amCores</span>)
appContext.<span class="fu">setResource</span>(capability)</code></pre>
<p>1.2: monitorApplication</p>
<pre class="sourceCode scala"><code class="sourceCode scala"><span class="kw">val</span> report = <span class="fu">getApplicationReport</span>(appId)
<span class="kw">val</span> state = report.<span class="fu">getYarnApplicationState</span>
<span class="kw">val</span> details = Seq[(String, String)](
      (<span class="st">&quot;client token&quot;</span>, <span class="fu">getClientToken</span>(report)),
      (<span class="st">&quot;diagnostics&quot;</span>, report.<span class="fu">getDiagnostics</span>),
      (<span class="st">&quot;ApplicationMaster host&quot;</span>, report.<span class="fu">getHost</span>),
      (<span class="st">&quot;ApplicationMaster RPC port&quot;</span>, report.<span class="fu">getRpcPort</span>.<span class="fu">toString</span>),
      (<span class="st">&quot;queue&quot;</span>, report.<span class="fu">getQueue</span>),
      (<span class="st">&quot;start time&quot;</span>, report.<span class="fu">getStartTime</span>.<span class="fu">toString</span>),
      (<span class="st">&quot;final status&quot;</span>, report.<span class="fu">getFinalApplicationStatus</span>.<span class="fu">toString</span>),
      (<span class="st">&quot;tracking URL&quot;</span>, report.<span class="fu">getTrackingUrl</span>),
      (<span class="st">&quot;user&quot;</span>, report.<span class="fu">getUser</span>)
    )</code></pre>
<p>2: 启动ApplicationMaster</p>
<p>ApplicationMaster.main 是AM的入口函数</p>
<pre class="sourceCode scala"><code class="sourceCode scala"><span class="co">//读取参数并启动ApplicationMaster</span>
<span class="kw">val</span> amArgs = <span class="kw">new</span> <span class="fu">ApplicationMasterArguments</span>(args)
SparkHadoopUtil.<span class="fu">get</span>.<span class="fu">runAsSparkUser</span> { () =&gt;
  master = <span class="kw">new</span> <span class="fu">ApplicationMaster</span>(amArgs, <span class="kw">new</span> <span class="fu">YarnRMClient</span>(amArgs))
  System.<span class="fu">exit</span>(master.<span class="fu">run</span>())
}</code></pre>
<p>ApplicationMaster.run</p>
<pre class="sourceCode scala"><code class="sourceCode scala"><span class="co">//两种模式在这里分叉</span>
<span class="kw">if</span> (isClusterMode) {
    <span class="fu">runDriver</span>(securityMgr)
  } <span class="kw">else</span> {
    <span class="fu">runExecutorLauncher</span>(securityMgr)
}</code></pre>
<p>2.1: ApplicationMaster.runDriver</p>
<pre class="sourceCode scala"><code class="sourceCode scala"><span class="co">//配置IP Filter</span>
<span class="fu">addAmIpFilter</span>()

<span class="co">//启动用户程序</span>
userClassThread = <span class="fu">startUserApplication</span>()

<span class="co">//等待用户启动SC</span>
<span class="kw">val</span> sc = <span class="fu">waitForSparkContextInitialized</span>()

<span class="kw">if</span> (sc == <span class="kw">null</span>) {
  <span class="fu">finish</span>(FinalApplicationStatus.<span class="fu">FAILED</span>,
    ApplicationMaster.<span class="fu">EXIT_SC_NOT_INITED</span>,
    <span class="st">&quot;Timed out waiting for SparkContext.&quot;</span>)
} <span class="kw">else</span> {
  actorSystem = sc.<span class="fu">env</span>.<span class="fu">actorSystem</span>

  <span class="co">//启动AMAcotr</span>
  <span class="fu">runAMActor</span>(
    sc.<span class="fu">getConf</span>.<span class="fu">get</span>(<span class="st">&quot;spark.driver.host&quot;</span>),
    sc.<span class="fu">getConf</span>.<span class="fu">get</span>(<span class="st">&quot;spark.driver.port&quot;</span>),
    isClusterMode = <span class="kw">true</span>)

  <span class="co">//向RM注册AM相关信息</span>
  <span class="fu">registerAM</span>(sc.<span class="fu">ui</span>.<span class="fu">map</span>(_.<span class="fu">appUIAddress</span>).<span class="fu">getOrElse</span>(<span class="st">&quot;&quot;</span>), securityMgr)
  userClassThread.<span class="fu">join</span>()
}</code></pre>
<p>2.2: startUserApplication</p>
<pre class="sourceCode scala"><code class="sourceCode scala"><span class="co">//启动用户的程序</span>
<span class="kw">val</span> mainArgs = <span class="kw">new</span> Array[String](args.<span class="fu">userArgs</span>.<span class="fu">size</span>)
args.<span class="fu">userArgs</span>.<span class="fu">copyToArray</span>(mainArgs, <span class="dv">0</span>, args.<span class="fu">userArgs</span>.<span class="fu">size</span>)
mainMethod.<span class="fu">invoke</span>(<span class="kw">null</span>, mainArgs)
<span class="fu">finish</span>(FinalApplicationStatus.<span class="fu">SUCCEEDED</span>, ApplicationMaster.<span class="fu">EXIT_SUCCESS</span>)</code></pre>
<p>2.3: SparkContext</p>
<pre class="sourceCode scala"><code class="sourceCode scala"><span class="co">//用户的程序新建SparkContext</span>
<span class="co">//启动YarnClusterScheduler和YarnClusterSchedulerBackend</span>
<span class="kw">case</span> <span class="st">&quot;yarn-standalone&quot;</span> | <span class="st">&quot;yarn-cluster&quot;</span> =&gt;
    <span class="kw">if</span> (master == <span class="st">&quot;yarn-standalone&quot;</span>) {
      <span class="fu">logWarning</span>(
        <span class="st">&quot;</span><span class="ch">\&quot;</span><span class="st">yarn-standalone</span><span class="ch">\&quot;</span><span class="st"> is deprecated as of Spark 1.0. Use </span><span class="ch">\&quot;</span><span class="st">yarn-cluster</span><span class="ch">\&quot;</span><span class="st"> instead.&quot;</span>)
    }
    <span class="kw">val</span> scheduler = <span class="kw">try</span> {
      <span class="kw">val</span> clazz = Class.<span class="fu">forName</span>(<span class="st">&quot;org.apache.spark.scheduler.cluster.YarnClusterScheduler&quot;</span>)
      <span class="kw">val</span> cons = clazz.<span class="fu">getConstructor</span>(classOf[SparkContext])
      cons.<span class="fu">newInstance</span>(sc).<span class="fu">asInstanceOf</span>[TaskSchedulerImpl]
    } <span class="kw">catch</span> {
      <span class="kw">case</span> e: Exception =&gt; {
        <span class="kw">throw</span> <span class="kw">new</span> <span class="fu">SparkException</span>(<span class="st">&quot;YARN mode not available ?&quot;</span>, e)
      }
    }
    <span class="kw">val</span> backend = <span class="kw">try</span> {
      <span class="kw">val</span> clazz =
        Class.<span class="fu">forName</span>(<span class="st">&quot;org.apache.spark.scheduler.cluster.YarnClusterSchedulerBackend&quot;</span>)
      <span class="kw">val</span> cons = clazz.<span class="fu">getConstructor</span>(classOf[TaskSchedulerImpl], classOf[SparkContext])
      cons.<span class="fu">newInstance</span>(scheduler, sc).<span class="fu">asInstanceOf</span>[CoarseGrainedSchedulerBackend]
    } <span class="kw">catch</span> {
      <span class="kw">case</span> e: Exception =&gt; {
        <span class="kw">throw</span> <span class="kw">new</span> <span class="fu">SparkException</span>(<span class="st">&quot;YARN mode not available ?&quot;</span>, e)
      }
    }
    scheduler.<span class="fu">initialize</span>(backend)
    (backend, scheduler)</code></pre>
<p>2.4: YarnClusterSchedulerBackend -&gt; YarnSchedulerBackend -&gt; CoarseGrainedSchedulerBackend -&gt; SchedulerBackend<br />SchedulerBackend API</p>
<pre class="sourceCode scala"><code class="sourceCode scala"><span class="kw">def</span> <span class="fu">start</span>(): Unit
<span class="kw">def</span> <span class="fu">stop</span>(): Unit
<span class="kw">def</span> <span class="fu">reviveOffers</span>(): Unit
<span class="kw">def</span> <span class="fu">defaultParallelism</span>(): Int
<span class="kw">def</span> <span class="fu">killTask</span>(taskId: Long, executorId: String, interruptThread: Boolean): Unit
<span class="kw">def</span> <span class="fu">isReady</span>(): Boolean = <span class="kw">true</span>
<span class="kw">def</span> <span class="fu">applicationId</span>(): String = appId</code></pre>
<p>CoarseGrainedSchedulerBackend API</p>
<pre class="sourceCode scala"><code class="sourceCode scala"><span class="kw">def</span> <span class="fu">doRequestTotalExecutors</span>(requestedTotal: Int): Boolean
<span class="kw">def</span> <span class="fu">doKillExecutors</span>(executorIds: Seq[String]): Boolean
<span class="kw">def</span> <span class="fu">sufficientResourcesRegistered</span>(): Boolean</code></pre>
<p>YarnSchedulerBackend</p>
<pre class="sourceCode scala"><code class="sourceCode scala"><span class="co">//连接到ApplicationMaster中的AMActor，通过AMActor向Yarn进行资源申请</span>
<span class="kw">private</span> <span class="kw">val</span> yarnSchedulerActor: ActorRef =
actorSystem.<span class="fu">actorOf</span>(
  <span class="fu">Props</span>(<span class="kw">new</span> YarnSchedulerActor),
  name = YarnSchedulerBackend.<span class="fu">ACTOR_NAME</span>)

<span class="kw">override</span> <span class="kw">def</span> <span class="fu">doRequestTotalExecutors</span>(requestedTotal: Int): Boolean = {
    AkkaUtils.<span class="fu">askWithReply</span>[Boolean](
      <span class="fu">RequestExecutors</span>(requestedTotal), yarnSchedulerActor, askTimeout)
  }

<span class="kw">override</span> <span class="kw">def</span> <span class="fu">doKillExecutors</span>(executorIds: Seq[String]): Boolean = {
    AkkaUtils.<span class="fu">askWithReply</span>[Boolean](
      <span class="fu">KillExecutors</span>(executorIds), yarnSchedulerActor, askTimeout)
  }</code></pre>
<p>2.5: waitForSparkContextInitialized</p>
<pre class="sourceCode scala"><code class="sourceCode scala"><span class="co">//等待SC初始化完成</span>
<span class="kw">while</span> (sparkContextRef.<span class="fu">get</span>() == <span class="kw">null</span> &amp;&amp; System.<span class="fu">currentTimeMillis</span> &lt; deadline &amp;&amp; !finished) {
    <span class="fu">logInfo</span>(<span class="st">&quot;Waiting for spark context initialization ... &quot;</span>)
    sparkContextRef.<span class="fu">wait</span>(10000L)
  }</code></pre>
<p>3: AMActor</p>
<p>AMActor负责接收YarnSchedulerBackend的消息，来对Yarn进行资源申请。</p>
<p>ApplicationMaster.runAMActor</p>
<pre class="sourceCode scala"><code class="sourceCode scala">actor = actorSystem.<span class="fu">actorOf</span>(<span class="fu">Props</span>(<span class="kw">new</span> <span class="fu">AMActor</span>(driverUrl, isClusterMode)), name = <span class="st">&quot;YarnAM&quot;</span>)</code></pre>
<p>ApplicationMaster.AMActor</p>
<pre class="sourceCode scala"><code class="sourceCode scala"><span class="kw">override</span> <span class="kw">def</span> <span class="fu">preStart</span>() = {
  driver = context.<span class="fu">actorSelection</span>(driverUrl)
  <span class="co">// Send a hello message to establish the connection, after which</span>
  <span class="co">// we can monitor Lifecycle Events.</span>
  driver ! <span class="st">&quot;Hello&quot;</span>
  driver ! RegisterClusterManager
}

<span class="kw">override</span> <span class="kw">def</span> receive = {
  <span class="kw">case</span> x: DisassociatedEvent =&gt;
    <span class="co">// In cluster mode, do not rely on the disassociated event to exit</span>
    <span class="co">// This avoids potentially reporting incorrect exit codes if the driver fails</span>
    <span class="kw">if</span> (!isClusterMode) {
      <span class="fu">finish</span>(FinalApplicationStatus.<span class="fu">SUCCEEDED</span>, ApplicationMaster.<span class="fu">EXIT_SUCCESS</span>)
    }

  <span class="kw">case</span> x: AddWebUIFilter =&gt;
    driver ! x

  <span class="kw">case</span> <span class="fu">RequestExecutors</span>(requestedTotal) =&gt;
    Option(allocator) <span class="kw">match</span> {
      <span class="kw">case</span> Some(a) =&gt; a.<span class="fu">requestTotalExecutors</span>(requestedTotal)
      <span class="kw">case</span> None =&gt; <span class="fu">logWarning</span>(<span class="st">&quot;Container allocator is not ready to request executors yet.&quot;</span>)
    }
    sender ! <span class="kw">true</span>

  <span class="kw">case</span> <span class="fu">KillExecutors</span>(executorIds) =&gt;
    Option(allocator) <span class="kw">match</span> {
      <span class="kw">case</span> Some(a) =&gt; executorIds.<span class="fu">foreach</span>(a.<span class="fu">killExecutor</span>)
      <span class="kw">case</span> None =&gt; <span class="fu">logWarning</span>(<span class="st">&quot;Container allocator is not ready to kill executors yet.&quot;</span>)
    }
    sender ! <span class="kw">true</span>
}</code></pre>
<p>ApplicationMaster.registerAM</p>
<pre class="sourceCode scala"><code class="sourceCode scala"><span class="co">//向RM注册AM相关信息(UIAddress、HistoryAddress、SecurityManager、SecurityManager、preferredNodeLocation)，并启动线程申请资源</span>
allocator = client.<span class="fu">register</span>(yarnConf,
  <span class="kw">if</span> (sc != <span class="kw">null</span>) sc.<span class="fu">getConf</span> <span class="kw">else</span> sparkConf,
  <span class="kw">if</span> (sc != <span class="kw">null</span>) sc.<span class="fu">preferredNodeLocationData</span> <span class="kw">else</span> Map(),
  uiAddress,
  historyAddress,
  securityMgr)

<span class="co">//申请资源</span>
allocator.<span class="fu">allocateResources</span>()

<span class="co">//启动一个线程来向Yarn进行资源申请</span>
reporterThread = <span class="fu">launchReporterThread</span>()</code></pre>
<p>ApplicationMaster.launchReporterThread</p>
<pre class="sourceCode scala"><code class="sourceCode scala"><span class="co">//启动一个线程来向Yarn进行资源申请</span>
<span class="kw">if</span> (allocator.<span class="fu">getNumExecutorsFailed</span> &gt;= maxNumExecutorFailures) {
  <span class="fu">finish</span>(FinalApplicationStatus.<span class="fu">FAILED</span>,
    ApplicationMaster.<span class="fu">EXIT_MAX_EXECUTOR_FAILURES</span>,
    <span class="st">&quot;Max number of executor failures reached&quot;</span>)
} <span class="kw">else</span> {
  <span class="fu">logDebug</span>(<span class="st">&quot;Sending progress&quot;</span>)
  allocator.<span class="fu">allocateResources</span>()
}</code></pre>
<p>4: YarnAllocator</p>
<p>用来向Yarn进行资源申请</p>
<p>API</p>
<pre class="sourceCode scala"><code class="sourceCode scala"><span class="kw">def</span> getNumExecutorsRunning: Int = numExecutorsRunning
<span class="kw">def</span> getNumExecutorsFailed: Int = numExecutorsFailed
<span class="kw">def</span> getNumPendingAllocate: Int = <span class="fu">getNumPendingAtLocation</span>(ANY_HOST)

<span class="kw">def</span> <span class="fu">requestTotalExecutors</span>(requestedTotal: Int)
<span class="kw">def</span> <span class="fu">killExecutor</span>(executorId: String)
<span class="kw">def</span> <span class="fu">allocateResources</span>(): Unit
<span class="kw">def</span> <span class="fu">updateResourceRequests</span>(): Unit</code></pre>
<p>YarnAllocator.allocateResources</p>
<pre class="sourceCode scala"><code class="sourceCode scala"><span class="fu">updateResourceRequests</span>()
<span class="kw">val</span> allocateResponse = amClient.<span class="fu">allocate</span>(progressIndicator)
<span class="kw">val</span> allocatedContainers = allocateResponse.<span class="fu">getAllocatedContainers</span>()
<span class="kw">if</span> (allocatedContainers.<span class="fu">size</span> &gt; <span class="dv">0</span>) {
  <span class="fu">handleAllocatedContainers</span>(allocatedContainers)
}
<span class="kw">val</span> completedContainers = allocateResponse.<span class="fu">getCompletedContainersStatuses</span>()
<span class="kw">if</span> (completedContainers.<span class="fu">size</span> &gt; <span class="dv">0</span>) {
  <span class="fu">processCompletedContainers</span>(completedContainers)
}</code></pre>
<p>YarnAllocator.updateResourceRequests</p>
<pre class="sourceCode scala"><code class="sourceCode scala"><span class="kw">val</span> numPendingAllocate = getNumPendingAllocate
<span class="kw">val</span> missing = targetNumExecutors - numPendingAllocate - numExecutorsRunning

<span class="kw">if</span> (missing &gt; <span class="dv">0</span>) {
  <span class="kw">for</span> (i &lt;- <span class="dv">0</span> until missing) {
    <span class="kw">val</span> request = <span class="kw">new</span> <span class="fu">ContainerRequest</span>(resource, <span class="kw">null</span>, <span class="kw">null</span>, RM_REQUEST_PRIORITY)
    amClient.<span class="fu">addContainerRequest</span>(request)
    <span class="kw">val</span> nodes = request.<span class="fu">getNodes</span>
    <span class="kw">val</span> hostStr = <span class="kw">if</span> (nodes == <span class="kw">null</span> || nodes.<span class="fu">isEmpty</span>) <span class="st">&quot;Any&quot;</span> <span class="kw">else</span> nodes.<span class="fu">last</span>
  }
} <span class="kw">else</span> <span class="kw">if</span> (missing &lt; <span class="dv">0</span>) {
  <span class="kw">val</span> numToCancel = math.<span class="fu">min</span>(numPendingAllocate, -missing)
  <span class="kw">val</span> matchingRequests = amClient.<span class="fu">getMatchingRequests</span>(RM_REQUEST_PRIORITY, ANY_HOST, resource)
  <span class="kw">if</span> (!matchingRequests.<span class="fu">isEmpty</span>) {
    matchingRequests.<span class="fu">head</span>.<span class="fu">take</span>(numToCancel).<span class="fu">foreach</span>(amClient.<span class="fu">removeContainerRequest</span>)
  } <span class="kw">else</span> {
    <span class="fu">logWarning</span>(<span class="st">&quot;Expected to find pending requests, but found none.&quot;</span>)
  }
}</code></pre>
<p>5: AMRMClient[ContainerRequest]</p>
<p>Yarn提供的API，用于向Yarn申请资源。</p>
<p>YarnAllocator.getNumPendingAtLocation</p>
<pre class="sourceCode scala"><code class="sourceCode scala">amClient.<span class="fu">getMatchingRequests</span>(RM_REQUEST_PRIORITY, location, resource).<span class="fu">map</span>(_.<span class="fu">size</span>).<span class="fu">sum</span></code></pre>
<p>YarnAllocator.allocateResources</p>
<pre class="sourceCode scala"><code class="sourceCode scala">amClient.<span class="fu">allocate</span>(progressIndicator)</code></pre>
<p>YarnAllocator.updateResourceRequests</p>
<pre class="sourceCode scala"><code class="sourceCode scala"><span class="kw">val</span> request = <span class="kw">new</span> <span class="fu">ContainerRequest</span>(resource, <span class="kw">null</span>, <span class="kw">null</span>, RM_REQUEST_PRIORITY)
amClient.<span class="fu">addContainerRequest</span>(request)

amClient.<span class="fu">getMatchingRequests</span>(RM_REQUEST_PRIORITY, ANY_HOST, resource)
<span class="kw">if</span> (!matchingRequests.<span class="fu">isEmpty</span>) {
      matchingRequests.<span class="fu">head</span>.<span class="fu">take</span>(numToCancel).<span class="fu">foreach</span>(amClient.<span class="fu">removeContainerRequest</span>)
    } <span class="kw">else</span> {
      <span class="fu">logWarning</span>(<span class="st">&quot;Expected to find pending requests, but found none.&quot;</span>)
    }</code></pre>
<p>YarnAllocator.internalReleaseContainer</p>
<pre class="sourceCode scala"><code class="sourceCode scala">amClient.<span class="fu">releaseAssignedContainer</span>(container.<span class="fu">getId</span>())</code></pre>
<p>6: ExecutorRunnable</p>
<p>用来在Yarn的Container上启动程序</p>
<p>run</p>
<pre class="sourceCode scala"><code class="sourceCode scala">nmClient = NMClient.<span class="fu">createNMClient</span>()
nmClient.<span class="fu">init</span>(yarnConf)
nmClient.<span class="fu">start</span>()
startContainer</code></pre>
<p>startContainer</p>
<pre class="sourceCode scala"><code class="sourceCode scala"><span class="kw">val</span> ctx = Records.<span class="fu">newRecord</span>(classOf[ContainerLaunchContext])
  .<span class="fu">asInstanceOf</span>[ContainerLaunchContext]

<span class="kw">val</span> localResources = prepareLocalResources
ctx.<span class="fu">setLocalResources</span>(localResources)

ctx.<span class="fu">setEnvironment</span>(env)

<span class="kw">val</span> credentials = UserGroupInformation.<span class="fu">getCurrentUser</span>().<span class="fu">getCredentials</span>()
<span class="kw">val</span> dob = <span class="kw">new</span> <span class="fu">DataOutputBuffer</span>()
credentials.<span class="fu">writeTokenStorageToStream</span>(dob)
ctx.<span class="fu">setTokens</span>(ByteBuffer.<span class="fu">wrap</span>(dob.<span class="fu">getData</span>()))

<span class="kw">val</span> commands = <span class="fu">prepareCommand</span>(masterAddress, slaveId, hostname, executorMemory, executorCores,
  appId, localResources)

ctx.<span class="fu">setCommands</span>(commands)
ctx.<span class="fu">setApplicationACLs</span>(YarnSparkHadoopUtil.<span class="fu">getApplicationAclsForYarn</span>(securityMgr))

<span class="kw">if</span> (sparkConf.<span class="fu">getBoolean</span>(<span class="st">&quot;spark.shuffle.service.enabled&quot;</span>, <span class="kw">false</span>)) {
  <span class="kw">val</span> secretString = securityMgr.<span class="fu">getSecretKey</span>()
  <span class="kw">val</span> secretBytes =
    <span class="kw">if</span> (secretString != <span class="kw">null</span>) {
      <span class="co">// This conversion must match how the YarnShuffleService decodes our secret</span>
      JavaUtils.<span class="fu">stringToBytes</span>(secretString)
    } <span class="kw">else</span> {
      <span class="co">// Authentication is not enabled, so just provide dummy metadata</span>
      ByteBuffer.<span class="fu">allocate</span>(<span class="dv">0</span>)
    }
  ctx.<span class="fu">setServiceData</span>(Map[String, ByteBuffer](<span class="st">&quot;spark_shuffle&quot;</span> -&gt; secretBytes))
}

nmClient.<span class="fu">startContainer</span>(container, ctx)</code></pre>
<h3 id="yarn-client">Yarn-Client模式代码分析</h3>
<p>7: 触发提交Application的过程</p>
<p>用户新建SparkContext</p>
<pre class="sourceCode scala"><code class="sourceCode scala"><span class="co">//启动YarnClientSchedulerBackend</span>
<span class="kw">case</span> <span class="st">&quot;yarn-client&quot;</span> =&gt;
    <span class="kw">val</span> scheduler = <span class="kw">try</span> {
      <span class="kw">val</span> clazz =
        Class.<span class="fu">forName</span>(<span class="st">&quot;org.apache.spark.scheduler.cluster.YarnScheduler&quot;</span>)
      <span class="kw">val</span> cons = clazz.<span class="fu">getConstructor</span>(classOf[SparkContext])
      cons.<span class="fu">newInstance</span>(sc).<span class="fu">asInstanceOf</span>[TaskSchedulerImpl]

    } <span class="kw">catch</span> {
      <span class="kw">case</span> e: Exception =&gt; {
        <span class="kw">throw</span> <span class="kw">new</span> <span class="fu">SparkException</span>(<span class="st">&quot;YARN mode not available ?&quot;</span>, e)
      }
    }

    <span class="kw">val</span> backend = <span class="kw">try</span> {
      <span class="kw">val</span> clazz =
        Class.<span class="fu">forName</span>(<span class="st">&quot;org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend&quot;</span>)
      <span class="kw">val</span> cons = clazz.<span class="fu">getConstructor</span>(classOf[TaskSchedulerImpl], classOf[SparkContext])
      cons.<span class="fu">newInstance</span>(scheduler, sc).<span class="fu">asInstanceOf</span>[CoarseGrainedSchedulerBackend]
    } <span class="kw">catch</span> {
      <span class="kw">case</span> e: Exception =&gt; {
        <span class="kw">throw</span> <span class="kw">new</span> <span class="fu">SparkException</span>(<span class="st">&quot;YARN mode not available ?&quot;</span>, e)
      }
    }

    scheduler.<span class="fu">initialize</span>(backend)
    (backend, scheduler)</code></pre>
<p>YarnClientSchedulerBackend.start</p>
<pre class="sourceCode scala"><code class="sourceCode scala"><span class="kw">val</span> argsArrayBuf = <span class="kw">new</span> ArrayBuffer[String]()
argsArrayBuf += (<span class="st">&quot;--arg&quot;</span>, hostport)
argsArrayBuf ++= getExtraClientArguments

<span class="kw">val</span> args = <span class="kw">new</span> <span class="fu">ClientArguments</span>(argsArrayBuf.<span class="fu">toArray</span>, conf)
totalExpectedExecutors = args.<span class="fu">numExecutors</span>
client = <span class="kw">new</span> <span class="fu">Client</span>(args, conf)

<span class="co">//7.1</span>
appId = client.<span class="fu">submitApplication</span>()
<span class="co">//7.2</span>
<span class="fu">waitForApplication</span>()
<span class="co">//7.3</span>
<span class="fu">asyncMonitorApplication</span>()</code></pre>
<p>2: ApplicationMaster (和cluster模式稍有不同）</p>
<p>ExecutorLauncher.main</p>
<pre class="sourceCode scala"><code class="sourceCode scala"><span class="co">//Yarn-Client模式下，ApplicationMaster的入口</span>
<span class="co">//为什么不直接用ApplicationMaster.main？因为jps可以通过类的名字区分Client和Cluster模式</span>
<span class="kw">def</span> <span class="fu">main</span>(args: Array[String]) = {
  ApplicationMaster.<span class="fu">main</span>(args)
}</code></pre>
<p>ApplicationMaster.run</p>
<pre class="sourceCode scala"><code class="sourceCode scala"><span class="co">//这次选择runExecutorLauncher</span>
<span class="kw">if</span> (isClusterMode) {
  <span class="fu">runDriver</span>(securityMgr)
} <span class="kw">else</span> {
  <span class="fu">runExecutorLauncher</span>(securityMgr)
}</code></pre>
<p>2.6: ApplicationMaster.runExecutorLauncher</p>
<pre class="sourceCode scala"><code class="sourceCode scala">actorSystem = AkkaUtils.<span class="fu">createActorSystem</span>(<span class="st">&quot;sparkYarnAM&quot;</span>, Utils.<span class="fu">localHostName</span>, <span class="dv">0</span>,
  conf = sparkConf, securityManager = securityMgr).<span class="fu">_1</span>
<span class="co">//等待用户初始化SC</span>
<span class="fu">waitForSparkDriver</span>()
<span class="fu">addAmIpFilter</span>()
<span class="co">//向RM注册AM相关信息</span>
<span class="fu">registerAM</span>(sparkConf.<span class="fu">get</span>(<span class="st">&quot;spark.driver.appUIAddress&quot;</span>, <span class="st">&quot;&quot;</span>), securityMgr)
<span class="co">// In client mode the actor will stop the reporter thread.</span>
reporterThread.<span class="fu">join</span>()</code></pre>
<p>2.7: ApplicationMaster.waitForSparkDriver</p>
<pre class="sourceCode scala"><code class="sourceCode scala"><span class="kw">var</span> driverUp = <span class="kw">false</span>
<span class="kw">val</span> hostport = args.<span class="fu">userArgs</span>(<span class="dv">0</span>)
<span class="kw">val</span> (driverHost, driverPort) = Utils.<span class="fu">parseHostPort</span>(hostport)

<span class="kw">val</span> totalWaitTime = sparkConf.<span class="fu">getLong</span>(<span class="st">&quot;spark.yarn.am.waitTime&quot;</span>, 100000L)
<span class="kw">val</span> deadline = System.<span class="fu">currentTimeMillis</span> + totalWaitTime

<span class="kw">while</span> (!driverUp &amp;&amp; !finished &amp;&amp; System.<span class="fu">currentTimeMillis</span> &lt; deadline) {
  <span class="kw">try</span> {
    <span class="kw">val</span> socket = <span class="kw">new</span> Socket(driverHost, driverPort)
    socket.<span class="fu">close</span>()
    <span class="fu">logInfo</span>(<span class="st">&quot;Driver now available: %s:%s&quot;</span><span class="fu">.format</span>(driverHost, driverPort))
    driverUp = <span class="kw">true</span>
  } <span class="kw">catch</span> {
    <span class="kw">case</span> e: Exception =&gt;
      <span class="fu">logError</span>(<span class="st">&quot;Failed to connect to driver at %s:%s, retrying ...&quot;</span>.
        <span class="fu">format</span>(driverHost, driverPort))
      Thread.<span class="fu">sleep</span>(100L)
  }
}

<span class="kw">if</span> (!driverUp) {
  <span class="kw">throw</span> <span class="kw">new</span> <span class="fu">SparkException</span>(<span class="st">&quot;Failed to connect to driver!&quot;</span>)
}

sparkConf.<span class="fu">set</span>(<span class="st">&quot;spark.driver.host&quot;</span>, driverHost)
sparkConf.<span class="fu">set</span>(<span class="st">&quot;spark.driver.port&quot;</span>, driverPort.<span class="fu">toString</span>)

<span class="co">//启动AMActor</span>
<span class="fu">runAMActor</span>(driverHost, driverPort.<span class="fu">toString</span>, isClusterMode = <span class="kw">false</span>)</code></pre>
<h3 id="data-locality">Data Locality</h3>
<p>使用preferredNodeLocationData，可以让Yarn分配距离数据较近的Container</p>
<pre class="sourceCode scala"><code class="sourceCode scala"><span class="kw">val</span> locData = InputFormatInfo.<span class="fu">computePreferredLocations</span>(
  Seq(<span class="kw">new</span> <span class="fu">InputFormatInfo</span>(conf, classOf[TextInputFormat], <span class="kw">new</span> <span class="fu">Path</span>(<span class="st">&quot;hdfs:///myfile.txt&quot;</span>)))
<span class="kw">val</span> sc = <span class="kw">new</span> <span class="fu">SparkContext</span>(conf, locData)</code></pre>
<h3 id="spark-on-yarn">Spark on Yarn存在的问题</h3>
<ol>
<li>Spark无法动态增加/减少资源 <a href="https://issues.apache.org/jira/browse/YARN-1197">YARN-1197</a><br /></li>
<li>Spark日志存储问题 <a href="https://issues.apache.org/jira/browse/YARN-321">YARN-321</a><br /></li>
<li>YARN允许spark作业为driver和executor设置需要的cpu和内存资源量，但是到底设置多少最为合适，这显然不好确定。因此，最好能够提供一个资源获取工具，可以查看spark作业实际占用的内存和cpu资源量，以便修正用户的资源参数。</li>
</ol>
<h3 id="references">References</h3>
<ul>
<li><a href="http://my.oschina.net/u/1434348/blog/193374">YARN应用开发流程</a><br /></li>
<li><a href="http://www.chinastor.org/upload/2014-07/14070710043699.pdf">Spark on Yarn: a deep dive</a> - Sandy Ryza @Cloudera<br /></li>
<li><a href="http://dongxicheng.org/framework-on-yarn/spark-on-yarn-challenge/">spark on yarn的技术挑战</a> - 董的博客<br /></li>
<li><a href="http://blog.cloudera.com/blog/2014/05/apache-spark-resource-management-and-yarn-app-models/">Apache Spark Resource Management and YARN App Models</a> - Cloudera Blog<br /></li>
<li><a href="http://www.cnblogs.com/hseagle/p/3728713.html">Apache Spark源码走读之8 -- Spark on Yarn</a> - 徽沪一郎的博客</li>
</ul>


  </div>
</div>

<!--highlight begin-->
<script type="text/javascript">
$(document).ready(function() {
    $('pre code').each(function(i, block) {
      hljs.highlightBlock(block);
    });
  });
</script>
<!--highlight end-->

<!--disques begin-->
<!--
<div id="disqus_thread"></div>
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES * * */
    var disqus_shortname = 'mars-notes';

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>
-->
<!--disques end-->

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-72149435-1', 'auto');
  ga('send', 'pageview');
</script>

</body>
</html>
