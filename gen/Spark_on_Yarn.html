<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">
<head>
   <meta http-equiv="content-type" content="text/html; charset=utf-8" />
   <meta name="author" content="Mars Gu" />
   <link href="http://kevinburke.bitbucket.org/markdowncss/markdown.css" rel="stylesheet"></link>
</head>
<body>
<h1>Spark on Yarn</h1>
<h3>Links for Yarn</h3>
<ul>
<li><a href="http://my.oschina.net/u/1434348/blog/193374">YARN应用开发流程</a></li>
</ul>
<h3>Links for Spark on Yarn</h3>
<ul>
<li><a href="http://dongxicheng.org/framework-on-yarn/spark-on-yarn-challenge/">spark on yarn的技术挑战</a> - 董的博客</li>
<li><a href="http://www.chinastor.org/upload/2014-07/14070710043699.pdf">Spark on Yarn: a deep dive</a> - Sandy Ryza @Cloudera</li>
<li><a href="http://blog.cloudera.com/blog/2014/05/apache-spark-resource-management-and-yarn-app-models/">Apache Spark Resource Management and YARN App Models</a></li>
<li><a href="http://www.cnblogs.com/hseagle/p/3728713.html">Apache Spark源码走读之8 -- Spark on Yarn</a></li>
</ul>
<h3>Data Locality</h3>
<p>使用preferredNodeLocationData</p>
<pre><code>val locData = InputFormatInfo.computePreferredLocations(
  Seq(new InputFormatInfo(conf, classOf[TextInputFormat], new Path("hdfs:///myfile.txt")))
val sc = new SparkContext(conf, locData)
</code></pre>
<h3>Problems</h3>
<ol>
<li>Spark无法动态增加/减少资源，Container-Resizing <a href="https://issues.apache.org/jira/browse/YARN-1197">YARN-1197</a></li>
<li><a href="https://issues.apache.org/jira/browse/YARN-321">Timeline Store YARN-321</a></li>
</ol>
<h3>Yarn-Cluster模式</h3>
<p>1: 提交Application</p>
<p>Client.submitApplication</p>
<pre><code>// Get a new application from our RM
val newApp = yarnClient.createApplication()
val newAppResponse = newApp.getNewApplicationResponse()
val appId = newAppResponse.getApplicationId()

// Set up the appropriate contexts to launch our AM
val containerContext = createContainerLaunchContext(newAppResponse)
val appContext = createApplicationSubmissionContext(newApp, containerContext)

// Finally, submit and monitor the application
yarnClient.submitApplication(appContext)
</code></pre>
<p>1.1: 创建environment, java options以及启动AM的命令</p>
<p>Client.createContainerLaunchContext</p>
<pre><code>val launchEnv = setupLaunchEnv(appStagingDir)
val amContainer = Records.newRecord(classOf[ContainerLaunchContext])
amContainer.setLocalResources(localResources)
amContainer.setEnvironment(launchEnv)

val amClass =
  if (isClusterMode) {
    Class.forName("org.apache.spark.deploy.yarn.ApplicationMaster").getName
  } else {
    Class.forName("org.apache.spark.deploy.yarn.ExecutorLauncher").getName
  }

// Command for the ApplicationMaster
val commands = prefixEnv ++ Seq(
    YarnSparkHadoopUtil.expandEnvironment(Environment.JAVA_HOME) + "/bin/java", "-server"
  ) ++
  javaOpts ++ amArgs ++
  Seq(
    "1&gt;", ApplicationConstants.LOG_DIR_EXPANSION_VAR + "/stdout",
    "2&gt;", ApplicationConstants.LOG_DIR_EXPANSION_VAR + "/stderr")
val printableCommands = commands.map(s =&gt; if (s == null) "null" else s).toList
amContainer.setCommands(printableCommands)

// send the acl settings into YARN to control who has access via YARN interfaces
val securityManager = new SecurityManager(sparkConf)
amContainer.setApplicationACLs(YarnSparkHadoopUtil.getApplicationAclsForYarn(securityManager))
setupSecurityToken(amContainer)
UserGroupInformation.getCurrentUser().addCredentials(credentials)
</code></pre>
<p>1.2: 创建提交AM的Context</p>
<p>Client.createApplicationSubmissionContext</p>
<pre><code>val appContext = newApp.getApplicationSubmissionContext
appContext.setApplicationName(args.appName)
appContext.setQueue(args.amQueue)
appContext.setAMContainerSpec(containerContext)
appContext.setApplicationType("SPARK")
sparkConf.getOption("spark.yarn.maxAppAttempts").map(_.toInt) match {
  case Some(v) =&gt; appContext.setMaxAppAttempts(v)
  case None =&gt; logDebug("spark.yarn.maxAppAttempts is not set. " +
      "Cluster's default value will be used.")
}
val capability = Records.newRecord(classOf[Resource])
capability.setMemory(args.amMemory + amMemoryOverhead)
capability.setVirtualCores(args.amCores)
appContext.setResource(capability)
</code></pre>
<p>2: 启动ApplicationMaster</p>
<p>ApplicationMaster.main</p>
<pre><code>val amArgs = new ApplicationMasterArguments(args)
SparkHadoopUtil.get.runAsSparkUser { () =&gt;
  master = new ApplicationMaster(amArgs, new YarnRMClient(amArgs))
  System.exit(master.run())
}
</code></pre>
<p>ApplicationMaster.run</p>
<pre><code>if (isClusterMode) {
    runDriver(securityMgr)
  } else {
    runExecutorLauncher(securityMgr)
  }
</code></pre>
<p>2.1: Cluster模式启动Driver</p>
<p>ApplicationMaster.runDriver</p>
<pre><code>addAmIpFilter()
userClassThread = startUserApplication()

// This a bit hacky, but we need to wait until the spark.driver.port property has
// been set by the Thread executing the user class.
val sc = waitForSparkContextInitialized()

// If there is no SparkContext at this point, just fail the app.
if (sc == null) {
  finish(FinalApplicationStatus.FAILED,
    ApplicationMaster.EXIT_SC_NOT_INITED,
    "Timed out waiting for SparkContext.")
} else {
  actorSystem = sc.env.actorSystem
  runAMActor(
    sc.getConf.get("spark.driver.host"),
    sc.getConf.get("spark.driver.port"),
    isClusterMode = true)
  registerAM(sc.ui.map(_.appUIAddress).getOrElse(""), securityMgr)
  userClassThread.join()
}
</code></pre>
<p>2.2 启动AMActor</p>
<p>ApplicationMaster.runAMActor</p>
<pre><code>actor = actorSystem.actorOf(Props(new AMActor(driverUrl, isClusterMode)), name = "YarnAM")
</code></pre>
<p>ApplicationMaster.AMActor</p>
<pre><code>override def preStart() = {
  driver = context.actorSelection(driverUrl)
  // Send a hello message to establish the connection, after which
  // we can monitor Lifecycle Events.
  driver ! "Hello"
  driver ! RegisterClusterManager
}

override def receive = {
  case x: DisassociatedEvent =&gt;
    // In cluster mode, do not rely on the disassociated event to exit
    // This avoids potentially reporting incorrect exit codes if the driver fails
    if (!isClusterMode) {
      finish(FinalApplicationStatus.SUCCEEDED, ApplicationMaster.EXIT_SUCCESS)
    }

  case x: AddWebUIFilter =&gt;
    driver ! x

  case RequestExecutors(requestedTotal) =&gt;
    Option(allocator) match {
      case Some(a) =&gt; a.requestTotalExecutors(requestedTotal)
      case None =&gt; logWarning("Container allocator is not ready to request executors yet.")
    }
    sender ! true

  case KillExecutors(executorIds) =&gt;
    Option(allocator) match {
      case Some(a) =&gt; executorIds.foreach(a.killExecutor)
      case None =&gt; logWarning("Container allocator is not ready to kill executors yet.")
    }
    sender ! true
}
</code></pre>
<p>2.3: registerAM</p>
<p>ApplicationMaster.registerAM</p>
<pre><code>allocator = client.register(yarnConf,
  if (sc != null) sc.getConf else sparkConf,
  if (sc != null) sc.preferredNodeLocationData else Map(),
  uiAddress,
  historyAddress,
  securityMgr)

allocator.allocateResources()
reporterThread = launchReporterThread()
</code></pre>
<p>2.4: launchReporterThread</p>
<p>ApplicationMaster.launchReporterThread</p>
<pre><code>if (allocator.getNumExecutorsFailed &gt;= maxNumExecutorFailures) {
  finish(FinalApplicationStatus.FAILED,
    ApplicationMaster.EXIT_MAX_EXECUTOR_FAILURES,
    "Max number of executor failures reached")
} else {
  logDebug("Sending progress")
  allocator.allocateResources()
}
</code></pre>
<p>3: 申请并启动Container
3.1: YarnAllocator</p>
<p>API</p>
<pre><code>def getNumExecutorsRunning: Int = numExecutorsRunning
def getNumExecutorsFailed: Int = numExecutorsFailed
def getNumPendingAllocate: Int = getNumPendingAtLocation(ANY_HOST)

def requestTotalExecutors(requestedTotal: Int)
def killExecutor(executorId: String)
def allocateResources(): Unit
def updateResourceRequests(): Unit
</code></pre>
<p>YarnAllocator.allocateResources</p>
<pre><code>updateResourceRequests()
val allocateResponse = amClient.allocate(progressIndicator)
val allocatedContainers = allocateResponse.getAllocatedContainers()
if (allocatedContainers.size &gt; 0) {
  handleAllocatedContainers(allocatedContainers)
}
val completedContainers = allocateResponse.getCompletedContainersStatuses()
if (completedContainers.size &gt; 0) {
  processCompletedContainers(completedContainers)
}
</code></pre>
<p>YarnAllocator.updateResourceRequests</p>
<pre><code>val numPendingAllocate = getNumPendingAllocate
val missing = targetNumExecutors - numPendingAllocate - numExecutorsRunning

if (missing &gt; 0) {
  for (i &lt;- 0 until missing) {
    val request = new ContainerRequest(resource, null, null, RM_REQUEST_PRIORITY)
    amClient.addContainerRequest(request)
    val nodes = request.getNodes
    val hostStr = if (nodes == null || nodes.isEmpty) "Any" else nodes.last
  }
} else if (missing &lt; 0) {
  val numToCancel = math.min(numPendingAllocate, -missing)
  val matchingRequests = amClient.getMatchingRequests(RM_REQUEST_PRIORITY, ANY_HOST, resource)
  if (!matchingRequests.isEmpty) {
    matchingRequests.head.take(numToCancel).foreach(amClient.removeContainerRequest)
  } else {
    logWarning("Expected to find pending requests, but found none.")
  }
}
</code></pre>
<p>3.2: AMRMClient[ContainerRequest]</p>
<p>getNumPendingAtLocation</p>
<pre><code>amClient.getMatchingRequests(RM_REQUEST_PRIORITY, location, resource).map(_.size).sum
</code></pre>
<p>allocateResources</p>
<pre><code>amClient.allocate(progressIndicator)
</code></pre>
<p>updateResourceRequests<br />
</p>
<pre><code>val request = new ContainerRequest(resource, null, null, RM_REQUEST_PRIORITY)
amClient.addContainerRequest(request)

amClient.getMatchingRequests(RM_REQUEST_PRIORITY, ANY_HOST, resource)
if (!matchingRequests.isEmpty) {
      matchingRequests.head.take(numToCancel).foreach(amClient.removeContainerRequest)
    } else {
      logWarning("Expected to find pending requests, but found none.")
    }
</code></pre>
<p>internalReleaseContainer</p>
<pre><code>amClient.releaseAssignedContainer(container.getId())
</code></pre>
<p>3.3: ExecutorRunnable</p>
<p>run</p>
<pre><code>nmClient = NMClient.createNMClient()
nmClient.init(yarnConf)
nmClient.start()
startContainer
</code></pre>
<p>startContainer</p>
<pre><code>val ctx = Records.newRecord(classOf[ContainerLaunchContext])
  .asInstanceOf[ContainerLaunchContext]

val localResources = prepareLocalResources
ctx.setLocalResources(localResources)

ctx.setEnvironment(env)

val credentials = UserGroupInformation.getCurrentUser().getCredentials()
val dob = new DataOutputBuffer()
credentials.writeTokenStorageToStream(dob)
ctx.setTokens(ByteBuffer.wrap(dob.getData()))

val commands = prepareCommand(masterAddress, slaveId, hostname, executorMemory, executorCores,
  appId, localResources)

ctx.setCommands(commands)
ctx.setApplicationACLs(YarnSparkHadoopUtil.getApplicationAclsForYarn(securityMgr))

if (sparkConf.getBoolean("spark.shuffle.service.enabled", false)) {
  val secretString = securityMgr.getSecretKey()
  val secretBytes =
    if (secretString != null) {
      // This conversion must match how the YarnShuffleService decodes our secret
      JavaUtils.stringToBytes(secretString)
    } else {
      // Authentication is not enabled, so just provide dummy metadata
      ByteBuffer.allocate(0)
    }
  ctx.setServiceData(Map[String, ByteBuffer]("spark_shuffle" -&gt; secretBytes))
}

nmClient.startContainer(container, ctx)
</code></pre>
</body>
</html>
