<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">
<head>
    <meta http-equiv="content-type" content="text/html; charset=utf-8" />
    <meta name="author" content="Mars Gu" />

    <title>Mars的笔记</title>

    <!--post-->
    <link href="http://marsishandsome.github.io/template/post.css" rel="stylesheet"></link>

    <script>
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

        ga('create', 'UA-72149435-1', 'auto');
        ga('send', 'pageview');
    </script>
</head>
<body>
<h1 id="spark1.5">Spark1.5</h1>
<h2 id="spark1.5.0">Spark1.5.0</h2>
<h3 id="new-experimental-user-defined-aggregate-function-udaf-interface">New experimental user-defined aggregate function (UDAF) interface</h3>
<ul>
<li><a href="https://issues.apache.org/jira/browse/SPARK-3947">SPARK-3947</a></li>
</ul>
<p>在SparkSQL中支持用户自定义的聚合函数，先register聚合函数，再在sql语句中调用。</p>
<pre><code>class GeometricMean extends UserDefinedAggregateFunction {...}
sqlContext.udf.register(&quot;gm&quot;, new GeometricMean)
sqlContext.sql(&quot;&quot;&quot;select gm(filedName) from tableName&quot;&quot;&quot;);</code></pre>
<h3 id="dataframe-hint-for-broadcast-join">DataFrame hint for broadcast join</h3>
<ul>
<li><a href="https://issues.apache.org/jira/browse/SPARK-8300">SPARK_8300</a></li>
</ul>
<p>根据表的大小是否小于<code>spark.sql.autoBroadcastJoinThreshold</code>，Spark会自动判断是否需要使用broadcast join，<br />新的语法可以让用户直接提示执行器使用broadcast join。</p>
<pre><code>left.join(broadcast(right), &quot;joinKey&quot;)</code></pre>
<h3 id="memory-and-local-disk-only-checkpointing-support">Memory and local disk only checkpointing support</h3>
<ul>
<li><a href="https://issues.apache.org/jira/browse/SPARK-1855">SPARK-1855</a><br /></li>
<li><a href="https://issues.apache.org/jira/secure/attachment/12741708/SPARK-7292-design.pdf">Design Doc</a></li>
</ul>
<p><code>rdd.checkpoint()</code>是基于DFS实现的，RDD的数据将会复制3份保持到硬盘，为了提高checkpoint的效率，<br />Spark提供了<code>rdd.localCheckpoint()</code>，Executor会把RDD的数据保持一份到本地磁盘，<br />提高了checkpoint的效率，但是当某个Executor挂掉时，该份数据将被重算。</p>
<h3 id="dynamic-allocation-in-yarn-works-with-preferred-locations">Dynamic allocation in YARN works with preferred locations</h3>
<ul>
<li><a href="https://issues.apache.org/jira/browse/SPARK-4352">SPARK-4352</a><br /></li>
<li><a href="https://issues.apache.org/jira/secure/attachment/12735126/Supportpreferrednodelocationindynamicallocation.pdf">Design Doc</a></li>
</ul>
<p>Spark会记录所有Pending Task的数据依赖，在向Yarn申请资源的时候，会尽量申请靠近数据的执行器。</p>
<h3 id="dynamic-resource-allocation-support-in-standalone-cluster">Dynamic resource allocation support in Standalone Cluster</h3>
<ul>
<li><a href="https://issues.apache.org/jira/browse/SPARK-4751">SPARK-4751</a></li>
</ul>
<p>之前Spark Standalone集群在启动任务前就需要把所需的执行器全部申请好，<br />现在支持运行起来以后动态增加执行器。</p>
<h3 id="faster-and-more-robust-dynamic-partition-insert">Faster and more robust dynamic partition insert</h3>
<ul>
<li><a href="https://issues.apache.org/jira/browse/SPARK-8890">SPARK-8890</a></li>
</ul>
<p>在使用Insert方式插入数据到HDFS的时候，每个执行器会打开所有需要插入的Partition的文件，<br />如果需要插入的文件数量很多，会导致执行器OOM。<br />该Patch只允许每个Executor一次最多打开<code>spark.sql.sources.maxConcurrentWrites</code>个文件，<br />默认值是5。如果大于这个数量，就对所需插入的数据进行外部排序，再依此进行插入。</p>
<h3 id="support-for-yarn-cluster-mode-in-r">Support for YARN cluster mode in R</h3>
<ul>
<li><a href="https://issues.apache.org/jira/browse/SPARK-6797">SPARK-6797</a></li>
</ul>
<p>SparkR在Yarn模式下，会把提交机器上<code>$SPARK_HOME/R/lib/SparkR</code>包括里面用户下载的RPackages<br />一起打包成sparkr.zip，并上传到Yarn上，使得Executors可以读取到相应的RPackages。</p>
<h3 id="spark-streaming-back-pressure">Spark Streaming Back Pressure</h3>
<ul>
<li><a href="https://issues.apache.org/jira/browse/SPARK-7398">SPARK-7398</a><br /></li>
<li><a href="http://www.wikiwand.com/en/PID_controller">PID</a></li>
</ul>
<p>因为Spark Streaming没有流量控制，在高峰期的时候每个Batch需要处理的数据会变多，很容易导致OOM问题。<br />增加了Back Pressure机制以后，Spark会控制每个Batch的数据量，避免OOM。</p>
<p>默认使用以下配置控制流量：</p>
<ul>
<li><code>spark.streaming.receiver.maxRate</code><br /></li>
<li><code>spark.streaming.kafka.maxRatePerPartition</code></li>
</ul>
<p>Spark还提供了更加动态的流量控制算法：PID</p>
<ul>
<li><code>spark.streaming.backpressure.rateEstimator=pid</code></li>
</ul>
<p>未来除了堆积的策略外，还会加入可配置的策略，例如：采样、丢弃等。</p>
<h3 id="better-load-balancing-and-scheduling-of-receivers-across-cluster">Better load balancing and scheduling of receivers across cluster</h3>
<ul>
<li><a href="https://issues.apache.org/jira/browse/SPARK-8882">SPARK-8882</a></li>
</ul>
<p>Receiver所在的Executor挂掉之后，新增调度算法来调度Receiver。</p>
<h2 id="spark1.5.1">Spark1.5.1</h2>
<h2 id="spark1.5.2">Spark1.5.2</h2>
<h2 id="references">References</h2>
<ul>
<li><a href="http://spark.apache.org/releases/spark-release-1-5-0.html">Spark1.5.0 Release Notes</a><br /></li>
<li><a href="http://spark.apache.org/releases/spark-release-1-5-1.html">Spark1.5.1 Release Notes</a><br /></li>
<li><a href="http://spark.apache.org/releases/spark-release-1-5-2.html">Spark1.5.2 Release Notes</a></li>
</ul>
</body>
</html>
