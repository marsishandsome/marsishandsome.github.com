<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">
<head>
   <meta http-equiv="content-type" content="text/html; charset=utf-8" />
   <meta name="author" content="Mars Gu" />
   <!--link href="http://kevinburke.bitbucket.org/markdowncss/markdown.css" rel="stylesheet"></link-->
   <link href="https://gist.githubusercontent.com/andyferra/2554919/raw/2e66cabdafe1c9a7f354aa2ebf5bc38265e638e5/github.css" rel="stylesheet"></link>
</head>
<body>
<h1 id="spark-job-server">Spark Job Server</h1>
<h3 id="zoomdata">Zoomdata</h3>
<ul>
<li><a href="http://spark-summit.org/wp-content/uploads/2015/03/SSE15-14-Zoomdata-Alarcon.pdf">Spark Summit</a></li>
</ul>
<p><img src="/images/spark_job_server_arch.png" width="1000px"></p>
<p><img src="/images/spark_job_server_remoting.png" /></p>
<h3 id="spark-job-server-1">Spark Job Server</h3>
<ul>
<li><a href="http://spark-summit.org/wp-content/uploads/2014/07/Spark-Job-Server-Easy-Spark-Job-Management-Chan-Chu.pdf">Spark Summit</a><br /></li>
<li><a href="https://github.com/spark-jobserver/spark-jobserver">Github New</a><br /></li>
<li><a href="http://github.com/ooyala/spark-jobserver">Github Old</a></li>
</ul>
<p><img src="/images/spark_job_server_spark_job_server.png" /></p>
<ol>
<li>&quot;Spark as a Service&quot;: Simple REST interface for all aspects of job, context management<br /></li>
<li>Support for Spark SQL Contexts/jobs and custom job contexts, Works for Spark Streaming as well!<br /></li>
<li>Supports sub-second low-latency jobs via long-running job contexts<br /></li>
<li>Start and stop job contexts for RDD sharing and low-latency jobs; change resources on restart<br /></li>
<li>Kill running jobs via stop context<br /></li>
<li>Separate jar uploading step for faster job startup<br /></li>
<li>Asynchronous and synchronous job API. Synchronous API is great for low latency jobs!<br /></li>
<li>Works with Standalone Spark as well as Mesos and yarn-client<br /></li>
<li>Job and jar info is persisted via a pluggable DAO interface<br /></li>
<li>Named RDDs to cache and retrieve RDDs by name, improving RDD sharing and reuse among jobs.<br /></li>
<li>Jars, job history and config are persisted via a pluggable API<br /></li>
<li>Async and sync API, JSON job results</li>
</ol>
<p>Example</p>
<pre><code>object WordCountExample extends SparkJob {
override def validate(sc: SparkContext, config: Config): SparkJobValidation = {
 Try(config.getString(“input.string”))
 .map(x =&gt; SparkJobValid)
 .getOrElse(SparkJobInvalid(“No input.string”))
 }
override def runJob(sc: SparkContext, config: Config): Any = {
 val dd = sc.parallelize(config.getString(“input.string”).split(&quot; &quot;).toSeq)
 dd.map((_, 1)).reduceByKey(_ + _).collect().toMap
 }
}</code></pre>
<h3 id="spark-kernel-ibm">Spark Kernel (IBM)</h3>
<ul>
<li><a href="https://github.com/ibm-et/spark-kernel">github</a></li>
</ul>
<p><img src="https://raw.githubusercontent.com/wiki/ibm-et/spark-kernel/overview.png" /></p>
<h3 id="hivethriftserver">HiveThriftServer</h3>
<ul>
<li><a href="https://github.com/apache/spark/tree/master/sql">github</a></li>
</ul>
<h3 id="spark-admin">Spark-Admin</h3>
<ul>
<li>provides administrators and developers a GUI to provision and manage Spark clusters easily<br /></li>
<li><a href="https://github.com/adatao/adatao-admin">github</a></li>
</ul>
<h3>二进制兼容问题</h3>
<ul>
<li>Reading from HDFS<br /></li>
<li>Spark driver’s hadoop libraries need to match those on the spark cluster<br /></li>
<li>We made these libraries configurable by:<br /></li>
<li>Decoupling the spark driver into a separate process<br /></li>
<li>Allowing administrators to configure what libraries through a UI<br /></li>
<li>Kicking off a spark driver process with the configured hadoop libraries</li>
</ul>

</body>
</html>
