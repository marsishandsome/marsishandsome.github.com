<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">
<head>
   <meta http-equiv="content-type" content="text/html; charset=utf-8" />
   <meta name="author" content="Mars Gu" />
   <link href="http://kevinburke.bitbucket.org/markdowncss/markdown.css" rel="stylesheet"></link>
</head>
<body>
<h1>Spark Job Server</h1>
<h3>Zoomdata</h3>
<ul>
<li><a href="http://spark-summit.org/wp-content/uploads/2015/03/SSE15-14-Zoomdata-Alarcon.pdf">Spark Summit</a></li>
</ul>
<img src="/images/spark_job_server_arch.png" width="800px">

<p><img alt="" src="/images/spark_job_server_remoting.png" /></p>
<h3>Spark Job Server</h3>
<ul>
<li><a href="http://spark-summit.org/wp-content/uploads/2014/07/Spark-Job-Server-Easy-Spark-Job-Management-Chan-Chu.pdf">Spark Summit</a></li>
<li><a href="https://github.com/spark-jobserver/spark-jobserver">Github New</a></li>
<li><a href="http://github.com/ooyala/spark-jobserver">Github Old</a></li>
</ul>
<p><img alt="" src="/images/spark_job_server_spark_job_server.png" /></p>
<ol>
<li>"Spark as a Service": Simple REST interface for all aspects of job, context management</li>
<li>Support for Spark SQL Contexts/jobs and custom job contexts, Works for Spark Streaming as well!</li>
<li>Supports sub-second low-latency jobs via long-running job contexts</li>
<li>Start and stop job contexts for RDD sharing and low-latency jobs; change resources on restart</li>
<li>Kill running jobs via stop context</li>
<li>Separate jar uploading step for faster job startup</li>
<li>Asynchronous and synchronous job API. Synchronous API is great for low latency jobs!</li>
<li>Works with Standalone Spark as well as Mesos and yarn-client</li>
<li>Job and jar info is persisted via a pluggable DAO interface</li>
<li>Named RDDs to cache and retrieve RDDs by name, improving RDD sharing and reuse among jobs.</li>
<li>Jars, job history and config are persisted via a pluggable API</li>
<li>Async and sync API, JSON job results</li>
</ol>
<p>Example</p>
<pre><code>object WordCountExample extends SparkJob {
override def validate(sc: SparkContext, config: Config): SparkJobValidation = {
 Try(config.getString(“input.string”))
 .map(x =&gt; SparkJobValid)
 .getOrElse(SparkJobInvalid(“No input.string”))
 }
override def runJob(sc: SparkContext, config: Config): Any = {
 val dd = sc.parallelize(config.getString(“input.string”).split(" ").toSeq)
 dd.map((_, 1)).reduceByKey(_ + _).collect().toMap
 }
}
</code></pre>
<h3>Spark Kernel (IBM)</h3>
<ul>
<li><a href="https://github.com/ibm-et/spark-kernel">github</a></li>
</ul>
<p><img alt="" src="https://raw.githubusercontent.com/wiki/ibm-et/spark-kernel/overview.png" /></p>
<h3>HiveThriftServer</h3>
<ul>
<li><a href="https://github.com/apache/spark/tree/master/sql">github</a></li>
</ul>
<h3>Spark-Admin</h3>
<ul>
<li>provides administrators and developers a GUI to provision and manage Spark clusters easily</li>
<li><a href="https://github.com/adatao/adatao-admin">github</a></li>
</ul>
<h3>二进制兼容问题</h3>
<ul>
<li>Reading from HDFS</li>
<li>Spark driver’s hadoop libraries need to match those on the spark cluster</li>
<li>We made these libraries configurable by:</li>
<li>Decoupling the spark driver into a separate process</li>
<li>Allowing administrators to configure what libraries through a UI</li>
<li>Kicking off a spark driver process with the configured hadoop libraries</li>
</ul>
</body>
</html>
