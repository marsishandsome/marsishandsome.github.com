<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">
<head>
   <meta http-equiv="content-type" content="text/html; charset=utf-8" />
   <meta name="author" content="Mars Gu" />
   <link href="http://kevinburke.bitbucket.org/markdowncss/markdown.css" rel="stylesheet"></link>
</head>
<body>
<h1>Spark Job Server</h1>
<h3>Zoomdata</h3>
<ul>
<li><a href="http://spark-summit.org/wp-content/uploads/2015/03/SSE15-14-Zoomdata-Alarcon.pdf">Spark Summit</a></li>
</ul>
<p><img alt="" src="/images/spark_job_server_arch.png" /></p>
<p><img alt="" src="/images/spark_job_server_remoting.png" /></p>
<ol>
<li>Fine grained configuration of Spark Context</li>
<li>Progress Reporting</li>
<li>Custom Parsers of Raw Values JdbcRdd.convertValue</li>
<li>TSV support</li>
<li>XML support</li>
<li>YARN support</li>
<li>Spark programming model</li>
<li>load and read requests in separate scheduler’s pools</li>
<li>Cancel queries</li>
</ol>
<h3>Spark Job Server</h3>
<ul>
<li><a href="http://spark-summit.org/wp-content/uploads/2014/07/Spark-Job-Server-Easy-Spark-Job-Management-Chan-Chu.pdf">Spark Summit</a></li>
<li><a href="https://github.com/spark-jobserver/spark-jobserver">Github New</a></li>
<li>
<p><a href="http://github.com/ooyala/spark-jobserver">Github Old</a></p>
</li>
<li>
<p>"Spark as a Service": Simple REST interface for all aspects of job, context management</p>
</li>
<li>Support for Spark SQL Contexts/jobs and custom job contexts, Works for Spark Streaming as well!</li>
<li>Supports sub-second low-latency jobs via long-running job contexts</li>
<li>Start and stop job contexts for RDD sharing and low-latency jobs; change resources on restart</li>
<li>Kill running jobs via stop context</li>
<li>Separate jar uploading step for faster job startup</li>
<li>Asynchronous and synchronous job API. Synchronous API is great for low latency jobs!</li>
<li>Works with Standalone Spark as well as Mesos and yarn-client</li>
<li>Job and jar info is persisted via a pluggable DAO interface</li>
<li>Named RDDs to cache and retrieve RDDs by name, improving RDD sharing and reuse among jobs.</li>
<li>Jars, job history and config are persisted via a pluggable API</li>
<li>Async and sync API, JSON job results</li>
</ul>
<p><code>object WordCountExample extends SparkJob {
override def validate(sc: SparkContext, config: Config): SparkJobValidation = {
 Try(config.getString(“input.string”))
 .map(x =&gt; SparkJobValid)
 .getOrElse(SparkJobInvalid(“No input.string”))
 }
override def runJob(sc: SparkContext, config: Config): Any = {
 val dd = sc.parallelize(config.getString(“input.string”).split(" ").toSeq)
 dd.map((_, 1)).reduceByKey(_ + _).collect().toMap
 }
}</code></p>
<p>sbt assembly -&gt; fat jar -&gt; upload to job server</p>
<p><code>curl --data-binary @job-server-tests/target/job-server-tests-$VER.jar localhost:8090/jars/test
OK</code></p>
<p>Ad-hoc Mode - Single, Unrelated Jobs (Transient Context)</p>
<p>```
curl -d "input.string = a b c a b see" 'localhost:8090/jobs?appName=test&amp;classPath=spark.jobserver.WordCountExample'
{
  "status": "STARTED",
  "result": {
    "jobId": "5453779a-f004-45fc-a11d-a39dae0f9bf4",
    "context": "b7ea0eb5-spark.jobserver.WordCountExample"
  }
}</p>
<p>curl localhost:8090/jobs/5453779a-f004-45fc-a11d-a39dae0f9bf4
{
  "status": "OK",
  "result": {
    "a": 2,
    "b": 2,
    "c": 1,
    "see": 1
  }
}
```</p>
<p>Persistent Context Mode - Faster &amp; Required for Related Jobs</p>
<p>```
curl -d "" 'localhost:8090/contexts/test-context?num-cpu-cores=4&amp;memory-per-node=512m'
OK</p>
<p>curl localhost:8090/contexts
["test-context"]</p>
<p>curl -d "input.string = a b c a b see" 'localhost:8090/jobs?appName=test&amp;classPath=spark.jobserver.WordCountExample&amp;context=test-context&amp;sync=true'
{
  "status": "OK",
  "result": {
    "a": 2,
    "b": 2,
    "c": 1,
    "see": 1
  }
}
```</p>
<h3>Spark Kernel (IBM)</h3>
<ul>
<li><a href="https://github.com/ibm-et/spark-kernel">github</a></li>
</ul>
<h3>HiveThriftServer</h3>
<h3>Spark-Admin -- provides administrators and developers a GUI to provision and manage Spark clusters easily</h3>
<ul>
<li><a href="https://github.com/adatao/adatao-admin">github</a></li>
</ul>
<h3>二进制兼容问题</h3>
<ul>
<li>Reading from HDFS</li>
<li>Spark driver’s hadoop libraries need to match those on the spark cluster</li>
<li>We made these libraries configurable by:</li>
<li>Decoupling the spark driver into a separate process</li>
<li>Allowing administrators to configure what libraries through a UI</li>
<li>Kicking off a spark driver process with the configured hadoop libraries</li>
</ul>
</body>
</html>
